{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Boosting is an ensemble learning technique that combines multiple weak learners sequentially to create a strong learner. Each weak learner corrects the errors of the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "- Reduces bias and variance.\n",
    "\n",
    "- Works well with weak learners.\n",
    "\n",
    "- Improves predictive accuracy.\n",
    "\n",
    "Limitations:\n",
    "- Prone to overfitting if not tuned properly.\n",
    "\n",
    "- Computationally expensive.\n",
    "\n",
    "- Sensitive to noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting works by training weak learners sequentially, adjusting sample weights to focus more on misclassified samples. The final prediction is made by combining all weak learners through weighted voting or averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost (Adaptive Boosting)\n",
    "\n",
    "Gradient Boosting (GBM)\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "LightGBM (Light Gradient Boosting Machine)\n",
    "\n",
    "CatBoost (Categorical Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_estimators: Number of weak learners.\n",
    "\n",
    "learning_rate: Shrinks the contribution of each weak learner.\n",
    "\n",
    "max_depth: Controls the complexity of weak learners.\n",
    "\n",
    "subsample: Fraction of samples used for training.\n",
    "\n",
    "loss: Loss function for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. How do boosting algorithms combine weak learners to create a strong learner?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting assigns weights to weak learners and combines their predictions using weighted voting (classification) or averaging (regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost assigns higher weights to misclassified samples and trains new models based on these weights. The final prediction is based on a weighted sum of weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential loss function:\n",
    "L = e^(-y f(x))\n",
    "where `y` is the true label and `f(x)` is the model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misclassified samples get higher weights, increasing their importance in the next iteration. The weight update formula:\n",
    "\n",
    "\n",
    "w_i = w_i * e^(alpha)\n",
    "\n",
    "where `alpha` depends on classifier performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Increases model complexity.\n",
    "\n",
    "- Improves accuracy (to a certain extent).\n",
    "\n",
    "- May lead to overfitting if too many estimators are used.-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
